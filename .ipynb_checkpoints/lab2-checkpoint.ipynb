{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r63x2M7tFQyV"
   },
   "source": [
    "# Deep Learning with TensorFlow/Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GvCRExfVFQyW"
   },
   "source": [
    "Now that we have completed a project of Machine Learning with spark ML, in this assignment, we will be swithing to the context of Deep Learning with Tensorflow/Keras by two tasks:\n",
    "- Task1: Image Classification with CNN\n",
    "- Task2: Image captioning with a combination of CNN and RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U7ILm5EaFQyX"
   },
   "source": [
    "## Task 1: Going Deeper with convolutions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vqZW8KZ-FQyY"
   },
   "source": [
    "Before **Inception v1** (**GoogLeNet**), which is the winner of the **ILSVRC** (ImageNet Large Scale Visual Recognition Competition) in 2014, most popular CNNs just stacked convolution layers deeper and deeper, hoping to get better performance.\n",
    "\n",
    "The Inception network, however, uses a lot of tricks to improve performance in terms of speed and accuracy.\n",
    "Compared to other networks, **Inception v1** has significant improvement over **ZFNet** (the winner in 2013) and **AlexNet** (the winner in 2012), and has relatively lower error rate compared with the VGGNet.\n",
    "\n",
    "In this task, we will be implementing the inception architecture [in this paper](https://arxiv.org/abs/1409.4842) with TensorFlow/Keras. \n",
    "\n",
    "The goal of this task is to understand how to write code to build the model, as long as you can verify the correctness of the code (e.g., through Keras model summary), it is not necessary to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ITjH3IZLFQyZ",
    "outputId": "f2f29667-7f8b-4844-d0ec-f89971805131"
   },
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "print(tensorflow.__version__)\n",
    "from tensorflow.keras import Model, layers, utils\n",
    "from tensorflow.nn import local_response_normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "fdV1UlAhVAm6",
    "outputId": "0210cca9-d9d7-4360-89f4-227f85c14539"
   },
   "outputs": [],
   "source": [
    "input_layer = layers.Input(shape=(224, 224, 3))\n",
    "\n",
    "intro_conv0_7x7 = layers.Conv2D(64, (7, 7), strides=(2, 2), padding=\"same\", activation=\"relu\")(input_layer)\n",
    "intro_pool0 = layers.MaxPool2D(pool_size=(3,3), strides=(2, 2), padding=\"same\")(intro_conv0_7x7)\n",
    "intro_norm0 = layers.Lambda(local_response_normalization)(intro_pool0)\n",
    "\n",
    "intro_conv1_1x1 = layers.Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")(intro_norm0)\n",
    "intro_conv1_3x3 = layers.Conv2D(192, (3, 3), padding=\"same\", activation=\"relu\")(intro_conv1_1x1)\n",
    "intro_pool1_3x3 = layers.MaxPool2D(pool_size=(3,3), strides=(2, 2), padding=\"same\")(intro_conv1_3x3)\n",
    "intro_norm1 = layers.Lambda(local_response_normalization)(intro_pool1_3x3)\n",
    "\n",
    "# inception3a module\n",
    "conv0_1x1 = layers.Conv2D(64, kernel_size=(1, 1), padding='same', activation='relu', name='inception3a_conv0_1x1')(intro_norm1)\n",
    "conv1_1x1 = layers.Conv2D(96, kernel_size=(1, 1), padding='same', activation='relu', name='inception3a_conv1_1x1')(intro_norm1)\n",
    "conv1_3x3 = layers.Conv2D(128, kernel_size=(3, 3), padding='same', activation='relu', name='inception3a_conv1_3x3')(conv1_1x1)\n",
    "conv2_1x1 = layers.Conv2D(16, kernel_size=(1, 1), padding='same', activation='relu', name='inception3a_conv2_1x1')(intro_norm1)\n",
    "conv2_5x5 = layers.Conv2D(32, kernel_size=(5, 5), padding='same', activation='relu', name='inception3a_conv2_5x5')(conv2_1x1)\n",
    "pool3_3x3 = layers.MaxPool2D(pool_size=(3, 3), strides=(1, 1), padding='same', name='inception3a_pool3_3x3')(intro_norm1)\n",
    "conv3_1x1 = layers.Conv2D(32, kernel_size=(5, 5), padding='same', activation='relu', name='inception3a_conv3_1x1')(pool3_3x3)\n",
    "inception3a = layers.Concatenate(name='inception3a')([conv0_1x1, conv1_3x3, conv2_5x5, conv3_1x1])\n",
    "\n",
    "# inception3b module\n",
    "inception3b_conv0_1x1 = layers.Conv2D(128, kernel_size=(1, 1), padding='same', activation='relu', name='inception3b_conv0_1x1')(inception3a)\n",
    "inception3b_conv1_1x1 = layers.Conv2D(128, kernel_size=(1, 1), padding='same', activation='relu', name='inception3b_conv1_1x1')(inception3a)\n",
    "inception3b_conv1_3x3 = layers.Conv2D(192, kernel_size=(3, 3), padding='same', activation='relu', name='inception3b_conv1_3x3')(inception3b_conv1_1x1)\n",
    "inception3b_conv2_1x1 = layers.Conv2D(32, kernel_size=(1, 1), padding='same', activation='relu', name='inception3b_conv2_1x1')(inception3a)\n",
    "inception3b_conv2_5x5 = layers.Conv2D(96, kernel_size=(5, 5), padding='same', activation='relu', name='inception3b_conv2_5x5')(inception3b_conv2_1x1)\n",
    "inception3b_pool3_3x3 = layers.MaxPool2D(pool_size=(3, 3), strides=(1, 1), padding='same', name='inception3b_pool3_3x3')(inception3a)\n",
    "inception3b_conv3_1x1 = layers.Conv2D(64, kernel_size=(5, 5), padding='same', activation='relu', name='inception3b_conv3_1x1')(inception3b_pool3_3x3)\n",
    "inception3b = layers.Concatenate(name='inception3b')([inception3b_conv0_1x1, inception3b_conv1_3x3, inception3b_conv2_5x5, inception3b_conv3_1x1])\n",
    "\n",
    "\n",
    "pool4a = layers.MaxPool2D(pool_size=(3,3), strides=(2, 2), padding=\"same\")(inception3b)\n",
    "# inception4a module\n",
    "inception4a_conv0_1x1 = layers.Conv2D(192, kernel_size=(1, 1), padding='same', activation='relu', name='inception4a_conv0_1x1')(pool4a)\n",
    "inception4a_conv1_1x1 = layers.Conv2D(96, kernel_size=(1, 1), padding='same', activation='relu', name='inception4a_conv1_1x1')(pool4a)\n",
    "inception4a_conv1_3x3 = layers.Conv2D(208, kernel_size=(3, 3), padding='same', activation='relu', name='inception4a_conv1_3x3')(inception4a_conv1_1x1)\n",
    "inception4a_conv2_1x1 = layers.Conv2D(16, kernel_size=(1, 1), padding='same', activation='relu', name='inception4a_conv2_1x1')(pool4a)\n",
    "inception4a_conv2_5x5 = layers.Conv2D(48, kernel_size=(5, 5), padding='same', activation='relu', name='inception4a_conv2_5x5')(inception4a_conv2_1x1)\n",
    "inception4a_pool3_3x3 = layers.MaxPool2D(pool_size=(3, 3), strides=(1, 1), padding='same', name='inception4a_pool3_3x3')(pool4a)\n",
    "inception4a_conv3_1x1 = layers.Conv2D(64, kernel_size=(5, 5), padding='same', activation='relu', name='inception4a_conv3_1x1')(inception4a_pool3_3x3)\n",
    "inception4a = layers.Concatenate(name='inception4a')([inception4a_conv0_1x1, inception4a_conv1_3x3, inception4a_conv2_5x5, inception4a_conv3_1x1])\n",
    "\n",
    "# inception4b module\n",
    "inception4b_conv0_1x1 = layers.Conv2D(160, kernel_size=(1, 1), padding='same', activation='relu', name='inception4b_conv0_1x1')(inception4a)\n",
    "inception4b_conv1_1x1 = layers.Conv2D(112, kernel_size=(1, 1), padding='same', activation='relu', name='inception4b_conv1_1x1')(inception4a)\n",
    "inception4b_conv1_3x3 = layers.Conv2D(224, kernel_size=(3, 3), padding='same', activation='relu', name='inception4b_conv1_3x3')(inception4b_conv1_1x1)\n",
    "inception4b_conv2_1x1 = layers.Conv2D(24, kernel_size=(1, 1), padding='same', activation='relu', name='inception4b_conv2_1x1')(inception4a)\n",
    "inception4b_conv2_5x5 = layers.Conv2D(64, kernel_size=(5, 5), padding='same', activation='relu', name='inception4b_conv2_5x5')(inception4b_conv2_1x1)\n",
    "inception4b_pool3_3x3 = layers.MaxPool2D(pool_size=(3, 3), strides=(1, 1), padding='same', name='inception4b_pool3_3x3')(inception4a)\n",
    "inception4b_conv3_1x1 = layers.Conv2D(64, kernel_size=(5, 5), padding='same', activation='relu', name='inception4b_conv3_1x1')(inception4b_pool3_3x3)\n",
    "inception4b = layers.Concatenate(name='inception4b')([inception4b_conv0_1x1, inception4b_conv1_3x3, inception4b_conv2_5x5, inception4b_conv3_1x1])\n",
    "\n",
    "# softmax after 4a\n",
    "inception4b_pool4_5x5 = layers.AveragePooling2D(pool_size=(5, 5), strides=(3, 3), padding='same', name='inception4b_pool4_5x5')(inception4a)\n",
    "inception4b_conv4_1x1 = layers.Conv2D(128, kernel_size=(1, 1), padding='same', activation='relu', name='inception4b_conv4_1x1')(inception4b_pool4_5x5)\n",
    "inception4b_dense4 = layers.Dense(1024, activation='relu', name='inception4b_dense4')(inception4b_conv4_1x1)\n",
    "inception4b_drop4 = layers.Dropout(0.7, name='inception4b_drop4')(inception4b_dense4)\n",
    "inception4b_classifier4 = layers.Dense(1000, name='inception4b_classifier4')(inception4b_drop4)\n",
    "inception4b_softmax = layers.Activation(activation='softmax')(inception4b_classifier4)\n",
    "\n",
    "# inception4c module\n",
    "inception4c_conv0_1x1 = layers.Conv2D(128, kernel_size=(1, 1), padding='same', activation='relu', name='inception4c_conv0_1x1')(inception4b)\n",
    "inception4c_conv1_1x1 = layers.Conv2D(128, kernel_size=(1, 1), padding='same', activation='relu', name='inception4c_conv1_1x1')(inception4b)\n",
    "inception4c_conv1_3x3 = layers.Conv2D(256, kernel_size=(3, 3), padding='same', activation='relu', name='inception4c_conv1_3x3')(inception4c_conv1_1x1)\n",
    "inception4c_conv2_1x1 = layers.Conv2D(24, kernel_size=(1, 1), padding='same', activation='relu', name='inception4c_conv2_1x1')(inception4b)\n",
    "inception4c_conv2_5x5 = layers.Conv2D(64, kernel_size=(5, 5), padding='same', activation='relu', name='inception4c_conv2_5x5')(inception4c_conv2_1x1)\n",
    "inception4c_pool3_3x3 = layers.MaxPool2D(pool_size=(3, 3), strides=(1, 1), padding='same', name='inception4c_pool3_3x3')(inception4b)\n",
    "inception4c_conv3_1x1 = layers.Conv2D(64, kernel_size=(5, 5), padding='same', activation='relu', name='inception4c_conv3_1x1')(inception4c_pool3_3x3)\n",
    "inception4c = layers.Concatenate(name='inception4c')([inception4c_conv0_1x1, inception4c_conv1_3x3, inception4c_conv2_5x5, inception4c_conv3_1x1])\n",
    "\n",
    "# inception4d module\n",
    "inception4d_conv0_1x1 = layers.Conv2D(112, kernel_size=(1, 1), padding='same', activation='relu', name='inception4d_conv0_1x1')(inception4c)\n",
    "inception4d_conv1_1x1 = layers.Conv2D(144, kernel_size=(1, 1), padding='same', activation='relu', name='inception4d_conv1_1x1')(inception4c)\n",
    "inception4d_conv1_3x3 = layers.Conv2D(288, kernel_size=(3, 3), padding='same', activation='relu', name='inception4d_conv1_3x3')(inception4d_conv1_1x1)\n",
    "inception4d_conv2_1x1 = layers.Conv2D(32, kernel_size=(1, 1), padding='same', activation='relu', name='inception4d_conv2_1x1')(inception4c)\n",
    "inception4d_conv2_5x5 = layers.Conv2D(64, kernel_size=(5, 5), padding='same', activation='relu', name='inception4d_conv2_5x5')(inception4d_conv2_1x1)\n",
    "inception4d_pool3_3x3 = layers.MaxPool2D(pool_size=(3, 3), strides=(1, 1), padding='same', name='inception4d_pool3_3x3')(inception4c)\n",
    "inception4d_conv3_1x1 = layers.Conv2D(64, kernel_size=(5, 5), padding='same', activation='relu', name='inception4d_conv3_1x1')(inception4d_pool3_3x3)\n",
    "inception4d = layers.Concatenate(name='inception4d')([inception4d_conv0_1x1, inception4d_conv1_3x3, inception4d_conv2_5x5, inception4d_conv3_1x1])\n",
    "\n",
    "# inception4e module\n",
    "inception4e_conv0_1x1 = layers.Conv2D(256, kernel_size=(1, 1), padding='same', activation='relu', name='inception4e_conv0_1x1')(inception4d)\n",
    "inception4e_conv1_1x1 = layers.Conv2D(160, kernel_size=(1, 1), padding='same', activation='relu', name='inception4e_conv1_1x1')(inception4d)\n",
    "inception4e_conv1_3x3 = layers.Conv2D(320, kernel_size=(3, 3), padding='same', activation='relu', name='inception4e_conv1_3x3')(inception4e_conv1_1x1)\n",
    "inception4e_conv2_1x1 = layers.Conv2D(32, kernel_size=(1, 1), padding='same', activation='relu', name='inception4e_conv2_1x1')(inception4d)\n",
    "inception4e_conv2_5x5 = layers.Conv2D(128, kernel_size=(5, 5), padding='same', activation='relu', name='inception4e_conv2_5x5')(inception4e_conv2_1x1)\n",
    "inception4e_pool3_3x3 = layers.MaxPool2D(pool_size=(3, 3), strides=(1, 1), padding='same', name='inception4e_pool3_3x3')(inception4d)\n",
    "inception4e_conv3_1x1 = layers.Conv2D(128, kernel_size=(5, 5), padding='same', activation='relu', name='inception4e_conv3_1x1')(inception4e_pool3_3x3)\n",
    "inception4e = layers.Concatenate(name='inception4e')([inception4e_conv0_1x1, inception4e_conv1_3x3, inception4e_conv2_5x5, inception4e_conv3_1x1])\n",
    "\n",
    "# softmax after 4d\n",
    "inception4e_pool4_5x5 = layers.AveragePooling2D(pool_size=(5, 5), strides=(3, 3), padding='same', name='inception4e_pool4_5x5')(inception4d)\n",
    "inception4e_conv4_1x1 = layers.Conv2D(128, kernel_size=(1, 1), padding='same', activation='relu', name='inception4e_conv4_1x1')(inception4e_pool4_5x5)\n",
    "inception4e_dense4 = layers.Dense(1024, activation='relu', name='inception4e_dense4')(inception4e_conv4_1x1)\n",
    "inception4e_drop4 = layers.Dropout(0.7, name='inception4e_drop4')(inception4e_dense4)\n",
    "inception4e_classifier4 = layers.Dense(1000, name='inception4e_classifier4')(inception4e_drop4)\n",
    "inception4e_softmax = layers.Activation(activation='softmax')(inception4e_classifier4)\n",
    "\n",
    "pool5a = layers.MaxPool2D(pool_size=(3,3), strides=(2, 2), padding=\"same\")(inception4e)\n",
    "# inception5a module\n",
    "inception5a_conv0_1x1 = layers.Conv2D(256, kernel_size=(1, 1), padding='same', activation='relu', name='inception5a_conv0_1x1')(pool5a)\n",
    "inception5a_conv1_1x1 = layers.Conv2D(160, kernel_size=(1, 1), padding='same', activation='relu', name='inception5a_conv1_1x1')(pool5a)\n",
    "inception5a_conv1_3x3 = layers.Conv2D(320, kernel_size=(3, 3), padding='same', activation='relu', name='inception5a_conv1_3x3')(inception5a_conv1_1x1)\n",
    "inception5a_conv2_1x1 = layers.Conv2D(32, kernel_size=(1, 1), padding='same', activation='relu', name='inception5a_conv2_1x1')(pool5a)\n",
    "inception5a_conv2_5x5 = layers.Conv2D(128, kernel_size=(5, 5), padding='same', activation='relu', name='inception5a_conv2_5x5')(inception5a_conv2_1x1)\n",
    "inception5a_pool3_3x3 = layers.MaxPool2D(pool_size=(3, 3), strides=(1, 1), padding='same', name='inception5a_pool3_3x3')(pool5a)\n",
    "inception5a_conv3_1x1 = layers.Conv2D(128, kernel_size=(5, 5), padding='same', activation='relu', name='inception5a_conv3_1x1')(inception5a_pool3_3x3)\n",
    "inception5a = layers.Concatenate(name='inception5a')([inception5a_conv0_1x1, inception5a_conv1_3x3, inception5a_conv2_5x5, inception5a_conv3_1x1])\n",
    "\n",
    "# inception5b module\n",
    "inception5b_conv0_1x1 = layers.Conv2D(384, kernel_size=(1, 1), padding='same', activation='relu', name='inception5b_conv0_1x1')(inception5a)\n",
    "inception5b_conv1_1x1 = layers.Conv2D(192, kernel_size=(1, 1), padding='same', activation='relu', name='inception5b_conv1_1x1')(inception5a)\n",
    "inception5b_conv1_3x3 = layers.Conv2D(384, kernel_size=(3, 3), padding='same', activation='relu', name='inception5b_conv1_3x3')(inception5b_conv1_1x1)\n",
    "inception5b_conv2_1x1 = layers.Conv2D(48, kernel_size=(1, 1), padding='same', activation='relu', name='inception5b_conv2_1x1')(inception5a)\n",
    "inception5b_conv2_5x5 = layers.Conv2D(128, kernel_size=(5, 5), padding='same', activation='relu', name='inception5b_conv2_5x5')(inception5b_conv2_1x1)\n",
    "inception5b_pool3_3x3 = layers.MaxPool2D(pool_size=(3, 3), strides=(1, 1), padding='same', name='inception5b_pool3_3x3')(inception5a)\n",
    "inception5b_conv3_1x1 = layers.Conv2D(128, kernel_size=(5, 5), padding='same', activation='relu', name='inception5b_conv3_1x1')(inception5b_pool3_3x3)\n",
    "inception5b = layers.Concatenate(name='inception5b')([inception5b_conv0_1x1, inception5b_conv1_3x3, inception5b_conv2_5x5, inception5b_conv3_1x1])\n",
    "\n",
    "final_pool = layers.AveragePooling2D(pool_size=(7, 7), strides=(1, 1), padding='same', name='final_pool')(inception5b)\n",
    "final_flat = layers.Flatten()(final_pool)\n",
    "final_drop = layers.Dropout(0.4, name='final_drop')(final_flat)\n",
    "final_classifier4 = layers.Dense(1000, name='final_classifier4')(final_drop)\n",
    "final_softmax = layers.Activation(activation='softmax')(final_classifier4)\n",
    "\n",
    "model = Model(input_layer, [final_softmax, inception4b_softmax, inception4e_softmax])\n",
    "model.compile('sgd', 'mse')\n",
    "model.summary()\n",
    "utils.plot_model(model, show_shapes=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-63nuP4xFQyc"
   },
   "source": [
    "## Task 2: Show and Tell: A Neural Image Caption Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "72WC_gmaFQyd"
   },
   "source": [
    "Automatically describing the content of an image is a fundamental problem in AI that connects *computer vision* and *natural language processing*.\n",
    "In this task, we will be looking into how we can use CNNs and RNNs to build an Image Caption Generator.\n",
    "\n",
    "\n",
    "Specifically, you will be implementing and training the model [in this paper](https://arxiv.org/abs/1411.4555) with TensorFlow/Keras on one of the datasets mentioned in the paper.\n",
    "\n",
    "To lighten the burden on training the network, you can use any pretrained network in [tf.keras.applications](https://www.tensorflow.org/api_docs/python/tf/keras/applications)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "colab_type": "code",
    "id": "xnNEdaj5eyGF",
    "outputId": "96f07ab2-7c8a-41ef-ffb4-4e1618854f33"
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Y7PcqNTLi3HZ",
    "outputId": "939eef7c-c58e-4b7e-fd5e-5651b7d16914"
   },
   "outputs": [],
   "source": [
    "!mkdir -p coco/images/train2017\n",
    "!gsutil -m rsync gs://images.cocodataset.org/train2017 coco/train2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "4sqvXVSvFQye",
    "outputId": "c7888f85-ee7c-435e-c782-9005cda67131"
   },
   "outputs": [],
   "source": [
    "!mkdir -p coco/images/val2017\n",
    "!gsutil -m rsync gs://images.cocodataset.org/val2017 coco/val2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "colab_type": "code",
    "id": "yjYpxOqsgnAA",
    "outputId": "30df9bdb-2128-42e1-825a-c28e18362930"
   },
   "outputs": [],
   "source": [
    "# get annotations\n",
    "!wget -q http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
    "!unzip annotations_trainval2017.zip -d coco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DceI6nRZfpTB"
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import time, random\n",
    "from pycocotools.coco import COCO\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Input, Dense, Dropout, LSTM, Embedding, concatenate\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "colab_type": "code",
    "id": "xytniqlyfrwF",
    "outputId": "fbf64600-336d-47bc-a056-2045ec36f073"
   },
   "outputs": [],
   "source": [
    "def load_captions(coco_imgToAnns): # coco.imgToAnns\n",
    "    captions = {}\n",
    "    max_caption_len = 0\n",
    "    for img_id, caption_list in tqdm(coco_imgToAnns.items()):\n",
    "        img_captions = []\n",
    "        for caption_md in caption_list:\n",
    "            caption = caption_md['caption']\n",
    "            caption_len = len(caption.split())\n",
    "            max_caption_len = caption_len if caption_len > max_caption_len else max_caption_len\n",
    "            img_captions.append(caption)\n",
    "        captions[img_id] = img_captions\n",
    "    return captions, max_caption_len\n",
    "\n",
    "def extract_features(model, model_input_size, img_folder_path, img_ids, file_format='jpg'):\n",
    "    # compute features\n",
    "    features = {}\n",
    "    for img_id in tqdm(img_ids):\n",
    "        img_path = \"{}/{}.{}\".format(img_folder_path, str(img_id).zfill(12), file_format)\n",
    "        feature = extract_img_features(model, model_input_size, img_path)\n",
    "        features[img_id] = feature\n",
    "    return features\n",
    "\n",
    "def extract_img_features(model, model_input_size, img_path):\n",
    "    img = load_img(img_path, target_size = model_input_size)\n",
    "    img = img_to_array(img)\n",
    "    img = img.reshape((1, img.shape[0], img.shape[1], img.shape[2]))\n",
    "    img = preprocess_input(img)\n",
    "    return model.predict(img)\n",
    "\n",
    "def create_tokenizer(captions):\n",
    "    caps = [y for x in captions.values() for y in x]\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(caps)\n",
    "    return tokenizer\n",
    "\n",
    "def generate_sequences(feature, captions, tokenizer, pad_max_len):\n",
    "    n_classes = len(tokenizer.word_index) + 1\n",
    "    sequences = []\n",
    "    for caption in captions:\n",
    "        sequence = tokenizer.texts_to_sequences([caption])[0]\n",
    "        for i in range(1, len(sequence)):\n",
    "            input_seq = pad_sequences([sequence[:i]], maxlen=pad_max_len)[0]\n",
    "            output_word = to_categorical([sequence[i]], num_classes=n_classes)[0]\n",
    "            sequences.append((feature, input_seq, output_word))\n",
    "    return sequences\n",
    "\n",
    "def create_generator(features, captions, tokenizer, max_caption_len, batch_size):\n",
    "    img_ids = list(captions.keys())\n",
    "    \n",
    "    n_imgs = len(img_ids)\n",
    "    n_batches = n_imgs // batch_size\n",
    "    if (n_imgs % batch_size != 0): n_batches += 1\n",
    "        \n",
    "    current_batch = 0\n",
    "    while True:\n",
    "        if current_batch >= n_batches: current_batch = 0 # restart when reach the end\n",
    "        input_features, input_seqs, output_words = list(), list(), list()\n",
    "        for i in range(current_batch * batch_size, min(n_imgs, current_batch * batch_size + batch_size)):\n",
    "            img_id = img_ids[i]\n",
    "            feature = features[img_id][0] # TODO- Check if features[img_id][0]\n",
    "            sequences = generate_sequences(feature, captions[img_id], tokenizer, max_caption_len)\n",
    "            for (input_feature, input_seq, output_word) in sequences:\n",
    "                input_features.append(input_feature)\n",
    "                input_seqs.append(input_seq)\n",
    "                output_words.append(output_word)\n",
    "        current_batch+=1\n",
    "        yield [[np.array(input_features), np.array(input_seqs)], np.array(output_words)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InceptionV3Model():\n",
    "    model = InceptionV3()\n",
    "    model_input_size = (299, 299)\n",
    "    model_output_size = (2048,)\n",
    "    \n",
    "    # pop classifier\n",
    "    model.layers.pop()\n",
    "    model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
    "    \n",
    "    return model, model_input_size, model_output_size\n",
    "\n",
    "def NICModel(img_size, txt_size, vocab_size, embedding_size=256, dropout_rate=0.5):\n",
    "    # image: Input -> Dropout -> Dense -> BatchNormalization -> Lambda (resize)\n",
    "    img_input = Input(shape=img_size)\n",
    "    img_l1 = Dropout(dropout_rate)(img_input)\n",
    "    img_model = Dense(embedding_size, activation='relu')(img_l1)    \n",
    "\n",
    "    # text: Input -> Embeddings -> Dropout\n",
    "    txt_input = Input(shape=(txt_size,))\n",
    "    txt_l1 = Embedding(vocab_size, embedding_size, mask_zero=True)(txt_input)\n",
    "    txt_l2 = Dropout(dropout_rate)(txt_l1)\n",
    "    \n",
    "    # lstm: \n",
    "    txt_model = LSTM(embedding_size)(txt_l2)\n",
    "    \n",
    "    # aggregate:\n",
    "    agg_model = concatenate([img_model, txt_model])\n",
    "    \n",
    "    # classification: Dense -> Dense // Dense -> Dropout -> Dense\n",
    "    cs_model_l1 = Dense(embedding_size, activation='relu')(agg_model)\n",
    "    cs_model = Dense(vocab_size, activation='softmax')(cs_model_l1)\n",
    "#     cs_model_l2 = Dropout(dropout_rate)(cs_model_l1)\n",
    "#     cs_model = Dense(vocab_size, activation='softmax')(cs_model_l2)\n",
    "    \n",
    "    # compilation\n",
    "    model = Model(inputs=[img_input, txt_input], outputs=cs_model)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caption generation with Beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_captions(feature, model, tokenizer, max_caption_len, beam_index=3):\n",
    "    initial_text = 'startseq'\n",
    "    input_seq = [[tokenizer.texts_to_sequences([initial_text])[0], 0.0]]\n",
    "    while len(input_seq[0][0]) < max_caption_len:\n",
    "        probs = []\n",
    "        for sequence in input_seq:\n",
    "            # predict next words\n",
    "            pad_sequence = pad_sequences([sequence[0]], maxlen=max_caption_len)\n",
    "            preds = model.predict([feature, pad_sequence])\n",
    "            # pick top beam_index elements\n",
    "            top_preds = np.argsort(preds[0])[-beam_index:] # Modify\n",
    "            \n",
    "            for word in top_preds:\n",
    "                next_sequence, prob = sequence[0][:], sequence[1]\n",
    "                next_sequence.append(word)\n",
    "                prob += preds[0][word]\n",
    "                probs.append([next_sequence, prob])\n",
    "        \n",
    "        input_seq = probs\n",
    "        input_seq = sorted(input_seq, reverse=False, key=lambda l: l[1])\n",
    "        input_seq = input_seq[-beam_index:]\n",
    "        \n",
    "    input_seq = input_seq[-1][0]\n",
    "    words = [word for i in input_seq for word, idx in tokenizer.word_index.items() if idx == i]\n",
    "    caption = []\n",
    "    for word in words:\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "        else:\n",
    "            caption.append(word)\n",
    "    caption.append('endseq')\n",
    "    return ' '.join(caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize COCO api for captions\n",
    "coco_dir='coco'\n",
    "val_dir='val2017'\n",
    "train_dir='train2017'\n",
    "ann_file = coco_dir + '/annotations/captions_{}.json'\n",
    "\n",
    "# limited\n",
    "train_coco = COCO(ann_file.format(train_dir))\n",
    "train_dir='train2017_limited' # TODOD: TEST\n",
    "train_img_ids_limited = [img_id for img_id in train_coco.imgToAnns.keys() if os.path.isfile(\"coco/\" + train_dir + \"/\" + str(img_id).zfill(12) + \".jpg\")]\n",
    "\n",
    "# limited\n",
    "val_coco = COCO(ann_file.format(val_dir))\n",
    "val_dir='val2017_limited' # TODOD: TEST\n",
    "val_img_ids_limited = [img_id for img_id in val_coco.imgToAnns.keys() if os.path.isfile(\"coco/\" + val_dir + \"/\" + str(img_id).zfill(12) + \".jpg\")]\n",
    "\n",
    "train_coco = COCO(ann_file.format(train_dir))\n",
    "val_coco = COCO(ann_file.format(val_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "FJOtB5q7laSQ",
    "outputId": "e1cce9c5-9780-41da-d9e0-2078b7ba1ce2"
   },
   "outputs": [],
   "source": [
    "# load captions\n",
    "print(\"Loading captions...\")\n",
    "# train_captions, max_caption_len = load_captions(train_coco.imgToAnns)\n",
    "# val_captions, _ = load_captions(val_coco.imgToAnns)\n",
    "\n",
    "# limited\n",
    "limited_imgToAnns = { key: train_coco.imgToAnns[key] for key in train_img_ids_limited }\n",
    "train_captions, max_caption_len = load_captions(limited_imgToAnns)\n",
    "limited_imgToAnns = { key: val_coco.imgToAnns[key] for key in val_img_ids_limited }\n",
    "val_captions, _ = load_captions(limited_imgToAnns)\n",
    "\n",
    "# shuffle captions to improve training\n",
    "train_img_ids = list(train_captions.keys())\n",
    "random.shuffle(train_img_ids)\n",
    "tmp_train_captions = {_id: train_captions[_id] for _id in train_img_ids}\n",
    "train_captions = tmp_train_captions\n",
    "\n",
    "# extract features\n",
    "print(\"Extracting features...\")\n",
    "img_folder_path = coco_dir + '/{}'\n",
    "\n",
    "cnn_model, cnn_model_input_size, cnn_model_output_size = InceptionV3Model()\n",
    "print(cnn_model.summary())\n",
    "\n",
    "train_features = extract_features(cnn_model, cnn_model_input_size, img_folder_path.format(train_dir), train_captions.keys())\n",
    "val_features = extract_features(cnn_model, cnn_model_input_size, img_folder_path.format(val_dir), val_captions.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration\n",
    "epochs = 20\n",
    "batch_size = 64\n",
    "\n",
    "train_steps = len(train_captions) // batch_size\n",
    "if len(train_captions) % batch_size != 0: train_steps += 1\n",
    "\n",
    "val_steps = len(val_captions) // batch_size\n",
    "if len(val_captions) % batch_size != 0: val_steps += 1\n",
    "\n",
    "# tokenizer\n",
    "tokenizer = create_tokenizer(train_captions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# create generators\n",
    "train_generator = create_generator(train_features, train_captions, tokenizer, max_caption_len, batch_size)\n",
    "val_generator = create_generator(val_features, val_captions, tokenizer, max_caption_len, batch_size)\n",
    "\n",
    "# fit and save model\n",
    "lstm_model = NICModel(cnn_model_output_size, max_caption_len, vocab_size, dropout_rate=0.3) # NOTE: cnn_model_output_size => output of CNN model. (InceptionV3)\n",
    "print(lstm_model.summary())\n",
    "lstm_model.fit_generator(train_generator, epochs=epochs, steps_per_epoch=train_steps, validation_data=val_generator,\n",
    "            validation_steps=val_steps, verbose=1)\n",
    "lstm_model.save('models/lstm_model_{}.h5'.format(int(round(time.time() * 1000))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, features, captions, tokenizer, max_caption_len, beam_index=3):\n",
    "    preds, gtruth = [], []\n",
    "    for img_id, caps in tqdm(captions.items()):\n",
    "        pred = generate_captions(features[img_id], model, tokenizer, max_caption_len, beam_index)\n",
    "        preds.append(pred.split())\n",
    "        gtruth.append([caption.split() for caption in caps])\n",
    "\n",
    "    # bleu_score weights\n",
    "    weights = [(1.0, 0, 0, 0), (0.5, 0.5, 0, 0), (0.3, 0.3, 0.3, 0), (0.25, 0.25, 0.25, 0.25)]\n",
    "    return [corpus_bleu(gtruth, preds, weights=w) for w in weights]\n",
    "\n",
    "# evaluate\n",
    "bleu_scores = evaluate(lstm_model, val_features, val_captions, tokenizer, max_caption_len, beam_index=3)\n",
    "\n",
    "print(\"BLEU SCORES\")\n",
    "for i, bs in enumerate(bleu_scores, start=1):\n",
    "    print('BLEU-{}: {}'.format(i, bs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "test_dir = '{}/test2017'.format(coco_dir, test_dir)\n",
    "n_test_imgs=8\n",
    "\n",
    "# coco test\n",
    "test_coco = COCO('coco/annotations/image_info_test2017.json')\n",
    "\n",
    "# download imgs\n",
    "for img in test_coco.loadImgs(test_coco.getImgIds()[:n_test_imgs]):\n",
    "    with open('{}/{}'.format(test_dir, img['file_name']), 'wb') as handle:\n",
    "        response = requests.get(img['coco_url'], stream=True)\n",
    "        if not response.ok: print(response)\n",
    "        for block in response.iter_content(1024):\n",
    "            if not block: break\n",
    "            handle.write(block)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "beam_index=3\n",
    "\n",
    "# generate captions\n",
    "for img_path in os.listdir(test_dir):\n",
    "    features = extract_img_features(cnn_model, cnn_model_input_size, test_dir + \"/\" + img_path)\n",
    "    generated_caption = generate_captions(features, lstm_model, tokenizer, max_caption_len, beam_index=beam_index)\n",
    "    caption = 'Caption: ' + generated_caption.split()[1].capitalize()\n",
    "    for x in generated_caption.split()[2:len(generated_caption.split())-1]:\n",
    "        caption = caption + ' ' + x\n",
    "    caption += '.'\n",
    "    # Show image and its caption\n",
    "    I = Image.open(test_dir + \"/\" + img_path, 'r')\n",
    "    plt.axis('off')\n",
    "    plt.imshow(I)\n",
    "    plt.savefig(test_dir + '_output/' + img_path)\n",
    "    plt.show()\n",
    "    print(\"Caption: {}\".format(caption))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "lab2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
